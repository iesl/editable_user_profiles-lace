"""
Evaluate the rankings generated by sentence similarity models.
"""
import sys
import os
import errno
import argparse
import statistics
import codecs
import json
import csv
import random
import collections
import comet_ml as cml
import matplotlib
import scipy.stats

matplotlib.use('Agg')
import matplotlib.pyplot as plt
import seaborn as sns
import numpy as np
from scipy import stats as scipystats
from sklearn import metrics as skmetrics

from . import rank_metrics as rm


def create_dir(dir_name):
    """
    Create the directory whose name is passed.
    :param dir_name: String saying the name of directory to create.
    :return: None.
    """
    # Create output directory if it doesnt exist.
    try:
        os.makedirs(dir_name)
        print('Created: {}.'.format(dir_name))
    except OSError as ose:
        # For the case of *file* by name of out_dir existing
        if (not os.path.isdir(dir_name)) and (ose.errno == errno.EEXIST):
            sys.stderr.write('IO ERROR: Could not create output directory\n')
            sys.exit(1)
        # If its something else you don't know; report it and exit.
        if ose.errno != errno.EEXIST:
            sys.stderr.write('OS ERROR: {:d}: {:s}: {:s}\n'.format(ose.errno,
                                                                   ose.strerror,
                                                                   dir_name))
            sys.exit(1)


def recall_at_k(ranked_rel, atk, max_total_relevant):
    """
    Compute recall at k.
    :param ranked_rel: list(int); ranked list of relevance judged data.
    :param atk: int; rank at which to compute metric.
    :param max_total_relevant: int; maximum relevant to consider in
        case there are more relevant in total.
    :return: recall: float.
    """
    total_relevant = sum(ranked_rel)
    total_relevant = min(max_total_relevant, total_relevant)
    relatk = sum(ranked_rel[:atk])
    if total_relevant > 0:
        recall_atk = float(relatk)/total_relevant
    else:
        recall_atk = 0.0
    return recall_atk
    

def compute_metrics(ranked_judgements, pr_atks, max_total_relevant=10, threshold_grade=None):
    """
    Given the ranked judgements compute precision, recall and f1.
    :param ranked_judgements: list(int); graded or binary relevances in rank order.
    :param pr_atks: list(int); the @K values to use for computing precision and recall.
    :param max_total_relevant: int; using this max_total_relevant because
        the Neves 2019 paper does as well. But for the pool evals based
        on citation data pass your own number as max_total_relevant.
    :param threshold_grade: int; Assuming 0-3 graded relevances, threshold at some point
        and convert graded to binary relevance. If passed also compute NDCG.
    :return:
    """
    # This is for the case of the tag evals where fewer than the full set is retrieved and
    # recall needs the correct max total relevant.
    if isinstance(ranked_judgements, tuple):
        ranked_judgements, max_total_relevant = ranked_judgements[0], ranked_judgements[1]
    ndcg = 0
    metrics = {}
    if isinstance(threshold_grade, int):
        graded_judgements = ranked_judgements
        ranked_judgements = [1 if rel >= threshold_grade else 0 for rel in graded_judgements]
        # Use the full set of candidate not the pr_atk.
        ndcg = rm.ndcg_at_k(np.clip(graded_judgements, a_min=0, a_max=3), len(ranked_judgements))
        ndcg_20 = rm.ndcg_at_k(np.clip(graded_judgements, a_min=0, a_max=3), 20)
        ndcg_5 = rm.ndcg_at_k(np.clip(graded_judgements, a_min=0, a_max=3), 5)
        if not isinstance(ranked_judgements, tuple):
            max_total_relevant = sum(ranked_judgements)
    for atk in pr_atks:
        recall = recall_at_k(ranked_rel=ranked_judgements,
                             atk=atk, max_total_relevant=max_total_relevant)
        precision = rm.precision_at_k(r=ranked_judgements, k=atk)
        f1 = 2*precision*recall/(precision + recall) if (precision + recall) > 0 else 0.0
        metrics[f'precision@{atk}'] = float(precision)
        metrics[f'recall@{atk}'] = float(recall)
        metrics[f'f1@{atk}'] = float(f1)
    r_precision = rm.r_precision(r=ranked_judgements)
    av_precision = rm.average_precision(r=ranked_judgements)
    reciprocal_rank = rm.mean_reciprocal_rank(rs=[ranked_judgements])
    metrics['total_relevant'] = max_total_relevant
    metrics['r_precision'] = float(r_precision)
    metrics['av_precision'] = float(av_precision)
    metrics['reciprocal_rank'] = float(reciprocal_rank)
    metrics['ndcg'] = float(ndcg)
    metrics['ndcg@20'] = float(ndcg_20)
    metrics['ndcg@5'] = float(ndcg_5)
    return metrics


def aggregate_metrics_rec(query_metrics):
    """
    Given metrics over individual queries aggregate over different
    queries. Simple average for now.
    :param query_metrics: dict(query_id: metrics_dict from compute_metrics)
    :return:
    """
    precision10, precision20, precision50, precision80, f120 = 0.0, 0.0, 0.0, 0.0, 0.0
    # precision10, precision20, precision50, f120 = 0.0, 0.0, 0.0, 0.0
    recall10, recall20, recall50, recall80 = 0.0, 0.0, 0.0, 0.0
    recall10, recall20, recall50 = 0.0, 0.0, 0.0
    av_precision, mrr, ndcg, r_precision = 0.0, 0.0, 0.0, 0.0
    ndcg_20, ndcg_5 = 0.0, 0.0
    for queri_id, metrics in query_metrics.items():
        recall10 += metrics['recall@10']
        recall20 += metrics['recall@20']
        recall50 += metrics['recall@50']
        recall80 += metrics['recall@80']
        precision10 += metrics['precision@10']
        precision20 += metrics['precision@20']
        precision50 += metrics['precision@50']
        precision80 += metrics['precision@80']
        f120 += metrics['f1@20']
        av_precision += metrics['av_precision']
        mrr += metrics['reciprocal_rank']
        r_precision += metrics['r_precision']
        if 'ndcg' in metrics:
            ndcg += metrics['ndcg']
            ndcg_20 += metrics['ndcg@20']
            ndcg_5 += metrics['ndcg@5']
    num_queries = len(query_metrics)
    aggmetrics = {
        'recall@10': recall10/num_queries,
        'recall@20': recall20/num_queries,
        'recall@50': recall50/num_queries,
        'recall@80': recall80/num_queries,
        'precision@10': precision10/num_queries,
        'precision@20': precision20/num_queries,
        'precision@50': precision50/num_queries,
        'precision@80': precision80/num_queries,
        'f1@20': f120/num_queries,
        'r_precision': r_precision/num_queries,
        'mean_av_precision': av_precision/num_queries,
        'mean_reciprocal_rank': mrr/num_queries,
        'ndcg': ndcg/num_queries,
        'ndcg@20': ndcg_20/num_queries,
        'ndcg@5': ndcg_5/num_queries
    }
    return aggmetrics


def aggregate_metrics(query_metrics):
    """
    Given metrics over individual queries aggregate over different
    queries. Simple average for now.
    :param query_metrics: dict(query_id: metrics_dict from compute_metrics)
    :return:
    """
    precision10, precision20, precision50, f120 = 0.0, 0.0, 0.0, 0.0
    recall10, recall20, recall50 = 0.0, 0.0, 0.0
    av_precision, mrr, ndcg, r_precision = 0.0, 0.0, 0.0, 0.0
    ndcg_20, ndcg_50 = 0.0, 0.0
    ndcg_pr_5, ndcg_pr_10, ndcg_pr_15, ndcg_pr_20, ndcg_pr_25 = 0.0, 0.0, 0.0, 0.0, 0.0
    for queri_id, metrics in query_metrics.items():
        recall10 += metrics['recall@10']
        recall20 += metrics['recall@20']
        recall50 += metrics['recall@50']
        # recall80 += metrics['recall@80']
        precision10 += metrics['precision@10']
        precision20 += metrics['precision@20']
        precision50 += metrics['precision@50']
        f120 += metrics['f1@20']
        av_precision += metrics['av_precision']
        mrr += metrics['reciprocal_rank']
        r_precision += metrics['r_precision']
        if 'ndcg' in metrics:
            ndcg += metrics['ndcg']
            ndcg_20 += metrics['ndcg@20']
            ndcg_50 += metrics['ndcg@50']
            ndcg_pr_5 += metrics['ndcg%5']
            ndcg_pr_10 += metrics['ndcg%10']
            ndcg_pr_15 += metrics['ndcg%15']
            ndcg_pr_20 += metrics['ndcg%20']
            ndcg_pr_25 += metrics['ndcg%25']
    num_queries = len(query_metrics)
    aggmetrics = {
        'recall@10': recall10/num_queries,
        'recall@20': recall20/num_queries,
        'recall@50': recall50/num_queries,
        'precision@10': precision10/num_queries,
        'precision@20': precision20/num_queries,
        'precision@50': precision50/num_queries,
        'f1@20': f120/num_queries,
        'r_precision': r_precision/num_queries,
        'mean_av_precision': av_precision/num_queries,
        'mean_reciprocal_rank': mrr/num_queries,
        'ndcg': ndcg/num_queries,
        'ndcg@20': ndcg_20/num_queries,
        'ndcg@50': ndcg_50/num_queries,
        'ndcg%5': ndcg_pr_5/num_queries,
        'ndcg%10': ndcg_pr_10/num_queries,
        'ndcg%15': ndcg_pr_15/num_queries,
        'ndcg%20': ndcg_pr_20/num_queries,
        'ndcg%25': ndcg_pr_25/num_queries
    }
    return aggmetrics


def read_unf_doc_relevances_big(data_path, run_path, dataset, method):
    """
    Read the gold data and the model rankings and the relevances for the
    model.
    :param data_path: string; directory with gold citations for test pids and rankings
        from baseline methods in subdirectories.
    :param run_path: string; directory with ranked candidates for baselines a subdir of
        data_path else is a model run.
    :param method: string; method with which ranks were created.
    :param dataset: string; eval dataset.
    :return: qpid2rankedcand_relevances: dict('qpid_facet': [relevances]);
        candidate gold relevances for the candidates in order ranked by the
        model.
    """
    gold_fname_ori = os.path.join(data_path, f'test-uid2anns-{dataset}.json')
    ranked_fname_ori = os.path.join(run_path, f'test-pid2pool-{dataset}-{method}-ranked.json')
    gold_fname_big = os.path.join(data_path, f'test-uid2anns-{dataset}-big.json')
    ranked_fname_big = os.path.join(run_path, f'test-pid2pool-{dataset}-{method}-big-ranked.json')
    # Load gold test data - read both the original test set and the big users.
    pid2rels_gold = {}
    for descr, gold_fname in [('ori', gold_fname_ori), ('big', gold_fname_big)]:
        with codecs.open(gold_fname, 'r', 'utf-8') as fp:
            pid2pool_source = json.load(fp)
            num_query = len(pid2pool_source)
            print(f'Gold query pids {descr}: {num_query}')
            for qpid, pool_rel in pid2pool_source.items():
                pool = pool_rel['cands']
                cands_rels = pool_rel['relevance_adju']
                pid2rels_gold['{:s}'.format(qpid)] = \
                    dict([(pid, rel) for pid, rel in zip(pool, cands_rels)])
    print(f'Gold query pids: {len(pid2rels_gold)}')
    # Load ranked predictions on test data with methods.
    qpid2rankedcand_relevances = {}
    for descr, ranked_fname in [('ori', ranked_fname_ori), ('big', ranked_fname_big)]:
        with codecs.open(ranked_fname, 'r', 'utf-8') as fp:
            pid2ranks = json.load(fp)
            print(f'Valid ranked query pids {descr}: {len(pid2ranks)}')
            for qpid, citranks in pid2ranks.items():
                candpids = [pid_score[0] for pid_score in citranks]
                cand_relevances = [pid2rels_gold[qpid][pid] for pid in candpids]
                qpid2rankedcand_relevances[qpid] = cand_relevances
    print(f'Valid ranked query pids: {len(qpid2rankedcand_relevances)}')
    return qpid2rankedcand_relevances


def read_unf_doc_relevances(data_path, run_path, dataset, method, ann_suffix = None):
    """
    Read the gold data and the model rankings and the relevances for the
    model.
    :param data_path: string; directory with gold citations for test pids and rankings
        from baseline methods in subdirectories.
    :param run_path: string; directory with ranked candidates for baselines a subdir of
        data_path else is a model run.
    :param method: string; method with which ranks were created.
    :param dataset: string; eval dataset.
    :return: qpid2rankedcand_relevances: dict('qpid_facet': [relevances]);
        candidate gold relevances for the candidates in order ranked by the
        model.
    """
    if ann_suffix:
        assert (ann_suffix in {'ccf', 'warms', 'wsccf', 'bids', 'assigns'})
        gold_fname_ori = os.path.join(data_path, f'test-uid2anns-{dataset}-{ann_suffix}.json')
        ranked_fname_ori = os.path.join(run_path, f'test-pid2pool-{dataset}-{method}-{ann_suffix}-ranked.json')
    else:
        gold_fname_ori = os.path.join(data_path, f'test-uid2anns-{dataset}.json')
        ranked_fname_ori = os.path.join(run_path, f'test-pid2pool-{dataset}-{method}-ranked.json')
    # Load gold test data - read both the original test set and the big users.
    pid2rels_gold = {}
    with codecs.open(gold_fname_ori, 'r', 'utf-8') as fp:
        pid2pool_source = json.load(fp)
        num_query = len(pid2pool_source)
        print(f'Gold query pids: {num_query}')
        for qpid, pool_rel in pid2pool_source.items():
            pool = pool_rel['cands']
            cands_rels = pool_rel['relevance_adju']
            pid2rels_gold['{:s}'.format(qpid)] = dict([(pid, rel) for pid, rel in zip(pool, cands_rels)])
    print(f'Gold query pids: {len(pid2rels_gold)}')
    # Load ranked predictions on test data with methods.
    qpid2rankedcand_relevances = {}
    with codecs.open(ranked_fname_ori, 'r', 'utf-8') as fp:
        pid2ranks = json.load(fp)
        print(f'Valid ranked query pids: {len(pid2ranks)}')
        for qpid, citranks in pid2ranks.items():
            candpids = [pid_score[0] for pid_score in citranks]
            cand_relevances = [pid2rels_gold[qpid][pid] for pid in candpids]
            qpid2rankedcand_relevances[qpid] = cand_relevances
    print(f'Valid ranked query pids: {len(qpid2rankedcand_relevances)}')
    return qpid2rankedcand_relevances


def read_unf_doc_relevance_scores(data_path, run_path, dataset, method, ann_suffix):
    """
    Read the gold relevances and the model scores for pearson correlation calculation.
    :param data_path: string; directory with gold citations for test pids and rankings
        from baseline methods in subdirectories.
    :param run_path: string; directory with ranked candidates for baselines a subdir of
        data_path else is a model run.
    :param method: string; method with which ranks were created.
    :param dataset: string; eval dataset.
    :return: qpid2rankedcand_relevances: dict('qpid_facet': [relevances]);
        candidate gold relevances for the candidates in order ranked by the
        model.
    """
    assert (ann_suffix in {'bids'})
    gold_fname_ori = os.path.join(data_path, f'test-uid2anns-{dataset}-{ann_suffix}.json')
    ranked_fname_ori = os.path.join(run_path, f'test-pid2pool-{dataset}-{method}-{ann_suffix}-ranked.json')
    # Load gold test data.
    pid2rels_gold = {}
    with codecs.open(gold_fname_ori, 'r', 'utf-8') as fp:
        pid2pool_source = json.load(fp)
        num_query = len(pid2pool_source)
        print(f'Gold query pids: {num_query}')
        for qpid, pool_rel in pid2pool_source.items():
            pool = pool_rel['cands']
            cands_rels = pool_rel['relevance_adju']
            pid2rels_gold[qpid] = dict([(pid, rel) for pid, rel in zip(pool, cands_rels)])
    print(f'Gold query pids: {len(pid2rels_gold)}')
    # Load ranked predictions on test data with methods.
    qpid2rankedcand_scores = {}
    with codecs.open(ranked_fname_ori, 'r', 'utf-8') as fp:
        pid2ranks = json.load(fp)
        print(f'Valid ranked query pids: {len(pid2ranks)}')
        for qpid, citranks in pid2ranks.items():
            sorted_scores = []
            non_zero_rels = []
            for pid_score in citranks:
                if pid2rels_gold[qpid][pid_score[0]] != 0:
                    sorted_scores.append(pid_score[1])
                    non_zero_rels.append(pid2rels_gold[qpid][pid_score[0]])
            non_zero_rels = list(sorted(non_zero_rels, reverse=True))
            qpid2rankedcand_scores[qpid] = (sorted_scores, non_zero_rels)
    print(f'Valid ranked query pids: {len(qpid2rankedcand_scores)}')
    return qpid2rankedcand_scores


def graded_eval_pool_rerank_unf(data_path, run_path, method, dataset, split, comet_exp_key, warm_start,
                                ann_suffix=None):
    """
    Evaluate the re-ranked pool for unfaceted data. Anns use graded relevance scores.
    :param data_path: string; directory with gold citations for test pids and rankings
        from baseline methods in subdirectories.
    :param run_path: string; directory with ranked candidates for baselines a subdir of
        data_path else is a model run.
    :param method: string; method with which ranks were created.
    :param dataset: string; eval dataset.
    :param split: string; {dev, test}
    :return:
    """
    ATKS = [10, 20, 50, 80]
    if warm_start:
        if method in {'contentcf', 'cfbpr', 'popular', 'cfals'}:
            ann_suffix = 'wsccf'
            qpid2rankedcand_relevances = read_unf_doc_relevances(data_path=data_path, run_path=run_path,
                                                                 dataset=dataset, method=method, ann_suffix=ann_suffix)
        else:
            ann_suffix = 'warms'
            qpid2rankedcand_relevances = read_unf_doc_relevances(data_path=data_path, run_path=run_path,
                                                                 dataset=dataset, method=method, ann_suffix=ann_suffix)
        with codecs.open(os.path.join(data_path, f'{dataset}-evaluation_splits-{ann_suffix}.json'), 'r', 'utf-8') as fp:
            eval_splits = json.load(fp)
    else:
        if dataset in {'citeulikea', 'citeuliket', 'tedrec'}:
            if method in {'contentcf'}:
                qpid2rankedcand_relevances = read_unf_doc_relevances(data_path=data_path, run_path=run_path,
                                                                     dataset=dataset, method=method, ann_suffix='ccf')
            else:
                if dataset in {'citeulikea', 'citeuliket'}:
                    qpid2rankedcand_relevances = read_unf_doc_relevances_big(data_path=data_path, run_path=run_path,
                                                                             dataset=dataset, method=method)
                else:
                    qpid2rankedcand_relevances = read_unf_doc_relevances(data_path=data_path, run_path=run_path,
                                                                         dataset=dataset, method=method)
            with codecs.open(os.path.join(data_path, f'{dataset}-evaluation_splits.json'), 'r', 'utf-8') as fp:
                eval_splits = json.load(fp)
        elif dataset in {'oriclr2019', 'oriclr2020', 'oruai2019'}:
            qpid2rankedcand_relevances = read_unf_doc_relevances(data_path=data_path, run_path=run_path,
                                                                 dataset=dataset, method=method, ann_suffix=ann_suffix)
            if ann_suffix in {'bids'}:
                qpid2ranked_scores = read_unf_doc_relevance_scores(data_path=data_path, run_path=run_path,
                                                                   dataset=dataset, method=method, ann_suffix=ann_suffix)
            with codecs.open(os.path.join(data_path, f'{dataset}-evaluation_splits-{ann_suffix}.json'), 'r',
                             'utf-8') as fp:
                eval_splits = json.load(fp)
            
    split_user_ids = eval_splits[split]
    print(f'EVAL SPLIT: {split}; Number of queries: {len(split_user_ids)}')
    # with codecs.open(os.path.join(data_path, f'{dataset}-queries-release.csv')) as csvfile:
    #     reader = csv.DictReader(csvfile)
    #     query_metadata = dict([(row['paper_id'], row) for row in reader])
    #
    
    # Go over test papers and compute metrics.
    all_metrics = {}
    user_spearman_rs = []
    too_few_bids = []
    min_num_bids = 1
    num_cands = 0.0
    num_queries = 0.0
    perq_out_fname = os.path.join(run_path, f'test-pid2pool-{dataset}-{method}-{split}-perq.csv')
    perq_file = codecs.open(perq_out_fname, 'w', 'utf-8')
    perq_csv = csv.DictWriter(perq_file, extrasaction='ignore',
                              fieldnames=['user_id', 'ndcg@5', 'ndcg@20',
                                          'recall@10', 'recall@20', 'recall@50', 'recall@80',
                                          'total_relevant'])
    perq_csv.writeheader()
    print('Precision and recall at rank: {:}'.format(ATKS))
    for user_id in split_user_ids:
        qranked_judgements = qpid2rankedcand_relevances[user_id]
        # Todo: look into this later.
        if dataset in {'citeulikea', 'citeuliket', 'tedrec'} or ann_suffix == 'assigns':
            threshold = 1
        else:
            threshold = 1
        all_metrics[user_id] = compute_metrics(qranked_judgements, pr_atks=ATKS,
                                               threshold_grade=threshold)
        if dataset in {'oriclr2019', 'oriclr2020', 'oruai2019'} and ann_suffix == 'bids':
            # Correlation bw the things people bid on as long as it has more than 3 bids.
            # Issue for second conditional: https://github.com/scipy/scipy/issues/12241
            if len(qpid2ranked_scores[user_id][0]) > min_num_bids and len(set(qpid2ranked_scores[user_id][1])) > 1:
                user_spearman_rs.append(scipystats.spearmanr(qpid2ranked_scores[user_id][0],
                                                             qpid2ranked_scores[user_id][1]).correlation)
            else:
                too_few_bids.append(user_id)
        num_cands += len(qranked_judgements)
        num_queries += 1
        user_metrics = {'user_id': user_id,
                        'ndcg@5': '{:.4f}'.format(all_metrics[user_id]['ndcg@5']),
                        'ndcg@20': '{:.4f}'.format(all_metrics[user_id]['ndcg@20']),
                        'recall@10': '{:.4f}'.format(all_metrics[user_id]['recall@10']),
                        'recall@20': '{:.4f}'.format(all_metrics[user_id]['recall@20']),
                        'recall@50': '{:.4f}'.format(all_metrics[user_id]['recall@50']),
                        'recall@80': '{:.4f}'.format(all_metrics[user_id]['recall@80']),
                        'total_relevant': all_metrics[user_id]['total_relevant']}
        perq_csv.writerow(user_metrics)
    aggmetrics = aggregate_metrics_rec(query_metrics=all_metrics)
    if dataset in {'oriclr2019', 'oriclr2020', 'oruai2019'} and ann_suffix == 'bids':
        aggmetrics['spearmanr'] = statistics.mean(user_spearman_rs)
    print('Wrote: {:s}'.format(perq_file.name))
    perq_file.close()
    print('Total queries: {:d}; Total candidates: {:d}'.format(int(num_queries), int(num_cands)))
    # Print metrics.
    if dataset in {'oriclr2019', 'oriclr2020', 'oruai2019'} and ann_suffix == 'bids':
        print('SpearmanR; NDCG@5; NDCG@20; Recall@10; Recall@20; Recall@50; Recall@80')
        print('{:.2f}, {:.2f}, {:.2f}, {:.2f}, {:.2f}, {:.2f}, {:.2f}\n'.format(aggmetrics['spearmanr'],
            aggmetrics['ndcg@5'] * 100, aggmetrics['ndcg@20'] * 100, aggmetrics['recall@10'] * 100,
            aggmetrics['recall@20'] * 100, aggmetrics['recall@50'] * 100, aggmetrics['recall@80'] * 100))
        print(f'Less than minimum {min_num_bids} bids: {len(too_few_bids)}')
    else:
        print('NDCG@5; NDCG@20; Recall@10; Recall@20; Recall@50; Recall@80')
        print('{:.2f}, {:.2f}, - , {:.2f}, {:.2f}, {:.2f}, {:.2f}\n'.format(
            aggmetrics['ndcg@5']*100, aggmetrics['ndcg@20']*100, aggmetrics['recall@10']*100,
            aggmetrics['recall@20']*100, aggmetrics['recall@50']*100, aggmetrics['recall@80']*100))


def compute_significance(perq_path1, method1, perq_path2, method2, dataset, num_baseline_comparisions=1):
    """
    Given the per query results for different methods on disk, compute
    the statistical significance of the diff between the two results for
    a hardcoded set of metrics and for all of the facets.
    :param num_baseline_comparisions: int; this indicates the number of comparisions being made
        to apply a Bonferroni correction.
    :return:
    """
    significance_level_5 = 0.05/num_baseline_comparisions
    # significance_level_10 = 0.10/num_baseline_comparisions
    # metrics = ['reciprocal_rank', 'recall@10', 'recall@20', 'recall@50', 'ndcg%5']
    metrics = ['ndcg@20', 'recall@20', 'recall@50']
    meth1_pid2metrics = {}
    try:
        fp = codecs.open(os.path.join(perq_path1, f'test-pid2pool-{dataset}-{method1}-test-perq.csv'), 'r', 'utf-8')
    except FileNotFoundError:
        fp = codecs.open(os.path.join(perq_path1, f'test-pid2pool-{dataset}-{method1}-test-perq.txt'), 'r', 'utf-8')
    reader = csv.DictReader(fp)
    for res_row in reader:
        meth1_pid2metrics[res_row['user_id']] = res_row
    meth2_pid2metrics = {}
    try:
        fp = codecs.open(os.path.join(perq_path2, f'test-pid2pool-{dataset}-{method2}-test-perq.csv'), 'r', 'utf-8')
    except FileNotFoundError:
        fp = codecs.open(os.path.join(perq_path2, f'test-pid2pool-{dataset}-{method2}-test-perq.txt'), 'r', 'utf-8')
    reader = csv.DictReader(fp)
    for res_row in reader:
        meth2_pid2metrics[res_row['user_id']] = res_row
    metric2pval = {}
    for metric in metrics:
        m1, m2 = [], []
        # I shouldnt have to do this intersection but because of some mismatches
        # in # queries per split I have to.
        method1_qpids = set(meth1_pid2metrics.keys())
        method2_qpids = set(meth2_pid2metrics.keys())
        intersection_qpids = set.intersection(method1_qpids, method2_qpids)
        for qpid in intersection_qpids:
            m1.append(float(meth1_pid2metrics[qpid][metric]))
            m2.append(float(meth2_pid2metrics[qpid][metric]))
        tval, pval = scipystats.ttest_ind(m1, m2, equal_var=False, nan_policy='propagate')
        metric2pval[metric] = pval
    print(f'metric: {metric}; meth1: {len(method1_qpids)}; meth2: {len(method2_qpids)}; '
          f'intersect: {len(intersection_qpids)}')
    print('Method 1: {:s}; Method 2: {:s}'.format(method1, method2))
    print('NDCG@20; Recall@20; Recall@50; P-Values:')
    print('0.05; Bonferroni corrected significance: {:.4f}'.format(significance_level_5))
    # print('{:}: {:.4f}, {:}: {:.4f}, {:}: {:.4f}, {:}: {:.4f}'.
    #       format(True if metric2pval['reciprocal_rank'] < significance_level_5 else False, metric2pval['reciprocal_rank'],
    #              True if metric2pval['recall@10'] < significance_level_5 else False, metric2pval['recall@10'],
    #              True if metric2pval['recall@20'] < significance_level_5 else False, metric2pval['recall@20'],
    #              True if metric2pval['recall@50'] < significance_level_5 else False, metric2pval['recall@50'],
    #              True if metric2pval['ndcg%5'] < significance_level_5 else False, metric2pval['ndcg%5']))
    print('{:}: {:.2f}, {:}: {:.2f}, {:}: {:.2f}'.
          format(True if metric2pval['ndcg@20'] <= significance_level_5 else False, metric2pval['ndcg@20'],
                 True if metric2pval['recall@20'] <= significance_level_5 else False, metric2pval['recall@20'],
                 True if metric2pval['recall@50'] <= significance_level_5 else False, metric2pval['recall@50']))
    print('')
    # print('0.10; Bonferroni corrected significance: {:.4f}'.format(significance_level_10))
    # print('{:}: {:.4f}, {:}: {:.4f}, {:}: {:.4f}\n'.
    #       format(True if metric2pval['recall@10'] < significance_level_10 else False, metric2pval['recall@10'],
    #              True if metric2pval['recall@20'] < significance_level_10 else False, metric2pval['recall@20'],
    #              True if metric2pval['recall@50'] < significance_level_10 else False, metric2pval['recall@50']))
    
    
def main():
    """
    Parse command line arguments and call all the above routines.
    :return:
    """
    parser = argparse.ArgumentParser()
    subparsers = parser.add_subparsers(dest='subcommand',
                                       help='The action to perform.')
    # Evaluate the re-ranked pools of documents.
    evaluate_pool_ranks = subparsers.add_parser('eval_pool_ranking')
    evaluate_pool_ranks.add_argument('--data_path', required=True,
                                     help='Input path where file with ranked candidates'
                                          'and gold data are if its a baseline method.')
    evaluate_pool_ranks.add_argument('--run_path',
                                     help='Input path where file with ranked candidates. Model run dir.')
    evaluate_pool_ranks.add_argument('--run_name', default=None,
                                     help='Base name of directory with specific model runs embeddings.s')
    evaluate_pool_ranks.add_argument('--experiment', required=True,
                                     choices=['specter', 'miswordbienc',
                                              'upsentconsent', 'upnfconsent', 'upnfkpenc',
                                              'contentcf', 'cfbpr', 'cfals', 'popular',
                                              'docsbmpnet1b', 'docsbmsmbert', 'docsbnlibert',
                                              'sentsbmpnet1b', 'sentsbmsmbert', 'sentsbnlibert'],
                                     help='The experiment to evaluate.')
    evaluate_pool_ranks.add_argument('--dataset', required=True,
                                     choices=['citeulikea', 'citeuliket', 'tedrec',
                                              'oriclr2019', 'oriclr2020', 'oruai2019'],
                                     help='The dataset to evaluate for.')
    evaluate_pool_ranks.add_argument('--warm_start', action="store_true", default=False,
                                     help='Should the eval be for warm start or cold start.')
    evaluate_pool_ranks.add_argument('--comet_exp_key', default=None,
                                     help='Hash for comet experiment run. Goto copy this correctly.')
    # Compute significance for methods.
    compute_sig = subparsers.add_parser('result_signf')
    compute_sig.add_argument('--dataset', required=True,
                             choices=['citeulikea', 'citeuliket', 'tedrec', 'oriclr2019', 'oriclr2020', 'oruai2019'],
                             help='The dataset to evaluate for.')
    compute_sig.add_argument('--run_name1', required=True, help='Main method being compared')
    compute_sig.add_argument('--method1', required=True, help='Method name.')
    cl_args = parser.parse_args()

    if cl_args.subcommand == 'eval_pool_ranking':
        run_path = cl_args.run_path
        if cl_args.dataset in {'citeulikea', 'citeuliket', 'tedrec'}:
            graded_eval_pool_rerank_unf(data_path=cl_args.data_path, method=cl_args.experiment,
                                        dataset=cl_args.dataset, run_path=run_path, split='dev',
                                        comet_exp_key=cl_args.comet_exp_key, warm_start=cl_args.warm_start)
            print('\n\n')
            graded_eval_pool_rerank_unf(data_path=cl_args.data_path, method=cl_args.experiment,
                                        dataset=cl_args.dataset, run_path=run_path, split='test',
                                        comet_exp_key=cl_args.comet_exp_key, warm_start=cl_args.warm_start)
        elif cl_args.dataset in {'oriclr2019', 'oriclr2020', 'oruai2019'}:
            graded_eval_pool_rerank_unf(data_path=cl_args.data_path, method=cl_args.experiment,
                                        dataset=cl_args.dataset, run_path=run_path, split='test',
                                        comet_exp_key=cl_args.comet_exp_key, warm_start=cl_args.warm_start,
                                        ann_suffix='bids')
            print('\n\n')
            graded_eval_pool_rerank_unf(data_path=cl_args.data_path, method=cl_args.experiment,
                                        dataset=cl_args.dataset, run_path=run_path, split='test',
                                        comet_exp_key=cl_args.comet_exp_key, warm_start=cl_args.warm_start,
                                        ann_suffix='assigns')
    elif cl_args.subcommand == 'result_signf':
        base_path = f'/work/smysore_umass_edu/2021-edit_expertise/datasets_raw/{cl_args.dataset}'
        method2s = ['specter']
        runnames = ['specter-2022_09_26-16_22_09-init']
        assert(len(method2s) == len(runnames))
        for method2, runname2 in zip(method2s, runnames):
            compute_significance(perq_path1=os.path.join(base_path, cl_args.method1, cl_args.run_name1),
                                 method1=cl_args.method1,
                                 perq_path2=os.path.join(base_path, method2, runname2),
                                 method2=method2,
                                 dataset=cl_args.dataset, num_baseline_comparisions=len(method2s))
    else:
        sys.stderr.write("Unknown action.")


if __name__ == '__main__':
    main()